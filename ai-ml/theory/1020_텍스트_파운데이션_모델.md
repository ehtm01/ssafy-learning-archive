# 0. 학습 시작: 파운데이션 모델

### 파운데이션 모델이란?
- 대량의 데이터를 기반으로 사전 학습된 대규모 AI 모델
- 다양한 작업에 범용적으로 활용할 수 있는 기초(foundation) 역할을 함

### We will learn: 파운데이션 모델의 주요 특징
- 기존 AI 모델과의 차이점
- 3가지 핵심 구성요소

### We will learn: 텍스트 파운데이션 모델 (거대 언어 모델)
- 거대 언어 모델의 특징

### 예시: 텍스트 생성 (ChatGPT)

![1020_텍스트_파운데이션_모델_2025-10-20-13-41-57](images/1020_텍스트_파운데이션_모델_2025-10-20-13-41-57.png)

### 예시: 비디오 생성 (SORA)

![1020_텍스트_파운데이션_모델_2025-10-20-13-42-16](images/1020_텍스트_파운데이션_모델_2025-10-20-13-42-16.png)

### 멀티모달 입력을 통해 멀티모달 출력 (GPT-4o)
- 멀티모달: 이미지, 비디오, 오디오, 텍스트

  ![1020_텍스트_파운데이션_모델_2025-10-20-13-42-59](images/1020_텍스트_파운데이션_모델_2025-10-20-13-42-59.png)

### 파운데이션 모델 이전
- 새로운 태스크를 해결하려면 해당 태스크에 대한 "**별도의 학습**" 필요

  ![1020_텍스트_파운데이션_모델_2025-10-20-13-43-33](images/1020_텍스트_파운데이션_모델_2025-10-20-13-43-33.png)

### 파운데이션 모델
- 새로운 태스크를 해결하려면 "**자세한 설명(프롬프트)**"을 입력해주는 것으로 충분
  - ChatGPT: 텍스트 파운데이션 모델(a.k.a 거대 언어 모델 or Large Language Model or LLM)

    ![1020_텍스트_파운데이션_모델_2025-10-20-13-45-48](images/1020_텍스트_파운데이션_모델_2025-10-20-13-45-48.png)
  
  - SORA: 비디오 파운데이션 모델

    ![1020_텍스트_파운데이션_모델_2025-10-20-13-46-12](images/1020_텍스트_파운데이션_모델_2025-10-20-13-46-12.png)

### 파운데이션 모델의 3가지 구성요소
- (1) 빅데이터
  - 인터넷에 존재하는 데이터 수가 기하급수적으로 증가

    ![1020_텍스트_파운데이션_모델_2025-10-20-13-46-44](images/1020_텍스트_파운데이션_모델_2025-10-20-13-46-44.png)

  - 딥러닝 기반 AI 모델은 학습 데이터가 늘어날 수록 성능이 증가

    ![1020_텍스트_파운데이션_모델_2025-10-20-13-47-29](images/1020_텍스트_파운데이션_모델_2025-10-20-13-47-29.png)
  
- (2) 자가 학습(Self-supervised Learning) 알고리즘
  - 사람이 정답을 알려줄 필요 없음
    - 예시: <u>다음 토큰 예측</u>(Next token predictoin)을 통한 텍스트 파운데이션 모델(거대 언어 모델) 학습

    ![1020_텍스트_파운데이션_모델_2025-10-20-13-48-26](images/1020_텍스트_파운데이션_모델_2025-10-20-13-48-26.png)

    ![1020_텍스트_파운데이션_모델_2025-10-20-13-49-03](images/1020_텍스트_파운데이션_모델_2025-10-20-13-49-03.png)

- (3) 어텐션(Attention) 기반 트랜스포머(Transformer) 모델
  - 더 많은 데이터를 학습할 수 있는 인공신경망 구조
    - 어텐션(Attention): 입력 데이터에서 중요한 부분에 '주의를 집중'하는 메커니즘
    - 트랜스포머(Transformer): 어텐션 메커니즘을 기반으로 한 신경망 구조

    ![1020_텍스트_파운데이션_모델_2025-10-20-13-50-31](images/1020_텍스트_파운데이션_모델_2025-10-20-13-50-31.png)

# 1. 텍스트 파운데이션 모델 살펴보기

## 1. 텍스트 파운데이션 모델이란?

### 파운데이션 모델의 3가지 구성 요소 in 언어 모델(Language Model)
- GPT-1, BERT와 같은 언어 모델에도 3가지 구성 요소가 이미 포함되어 있음
  - 그러나, 파운데이션 모델과 같은 능력을 보여주지 못함 -> **어떤 차이 때문일까?**

  ![1020_텍스트_파운데이션_모델_2025-10-20-13-51-50](images/1020_텍스트_파운데이션_모델_2025-10-20-13-51-50.png)

- GPT-2: 언어 모델이 추가 학습 없이도 텍스트 지시를 통해 새로운 태스크를 "어느 정도" 수행할 수 있음을 확인

  ![1020_텍스트_파운데이션_모델_2025-10-20-13-52-22](images/1020_텍스트_파운데이션_모델_2025-10-20-13-52-22.png)

    - 가장 큰 GPT-2 모델 조차도 **underfitting**된 결과를 보여줌 -> **모델 크기를 더 늘렸을 때 성능이 개선될 여지가 있음**

      ![1020_텍스트_파운데이션_모델_2025-10-20-13-53-07](images/1020_텍스트_파운데이션_모델_2025-10-20-13-53-07.png)

### 텍스트 파운데이션 모델(거대 언어 모델)의 특이점
- (1) 규모의 법칙(Scaling Law)
  - 더 많은 데이터, 큰 모델, 긴 학습 -> 더 좋은 성능

    ![1020_텍스트_파운데이션_모델_2025-10-20-13-53-57](images/1020_텍스트_파운데이션_모델_2025-10-20-13-53-57.png)

- (2) 창발성(Emergent Property)
  - 특정 규모를 넘어서면 갑자기 모델에서 발현되는 성질
    - 예시 #1. 인-컨텍스트 학습(In-context Learning): 주어진 설명과 예시만으로 새로운 태스크를 수월하게 수행
    - 예시 #2. 추론(reasoning) 능력

    ![1020_텍스트_파운데이션_모델_2025-10-20-13-55-14](images/1020_텍스트_파운데이션_모델_2025-10-20-13-55-14.png)

## 2. 거대 언어 모델 예시

### 텍스트 파운데이션 모델 (or 거대 언어 모델, LLM)
- 기존 대비, (1) 더 큰 모델(>7B)이 (2) 더 많은 데이터(>1T)에서 학습되어 창발성이 나타나기 시작한 언어 모델
  - 일반적으로 거대 언어모델은 GPT와 같이 다음 토큰 예측을 통해 많은 텍스트 데이터에서 사전 학습된 트랜스포머 기반 모델을 의미

    ![1020_텍스트_파운데이션_모델_2025-10-20-13-57-23](images/1020_텍스트_파운데이션_모델_2025-10-20-13-57-23.png)

### 폐쇄형(Closed) 거대 언어 모델
- 예시: ChatGPT(OpenAI), Claude(Anthropic), Gemini(Google)
  - 장점: 일반적으로 더 우수한 성능 및 최신 기능을 갖고 있으며 사용하기 쉬움

    ![1020_텍스트_파운데이션_모델_2025-10-20-13-58-24](images/1020_텍스트_파운데이션_모델_2025-10-20-13-58-24.png)

  - 단점
    - (1) 사용 시 마다 비용이 발생
    - (2) 모델이나 출력에 대한 정보가 제한적으로 제공됨

    ![1020_텍스트_파운데이션_모델_2025-10-20-13-59-05](images/1020_텍스트_파운데이션_모델_2025-10-20-13-59-05.png)

- ChatGPT(OpenAI): 가장 많은 활성 유저 수. 전반적으로 뛰어난 성능
- Claude(Anthropic): 안전 지향적 모델. 코딩 관련 작업에 특히 뛰어난 성능
- Gemini(Google): 가장 긴 입력 및 출력(>1M)을 지원. 뛰어난 멀티 모달 성능

![1020_텍스트_파운데이션_모델_2025-10-20-14-00-33](images/1020_텍스트_파운데이션_모델_2025-10-20-14-00-33.png)

### 개방형(Open Sourced) 거대 언어 모델
- 예시: LLaMA(Meta), Gemma(Google), Qwen(Alibaba)
  - 장점
    - (1) 무료로 다운로드 및 사용 가능
    - (2) 모든 정보(e.g. 모델 구조, 소스 코드)가 공개되어 있음
  - 단점
    - (1) 충분한 계산 자원 (e.g. GPU) 필요
    - (2) 상대적으로 폐쇄형 모델에 비해 성능이 낮은 편

    ![1020_텍스트_파운데이션_모델_2025-10-20-14-02-56](images/1020_텍스트_파운데이션_모델_2025-10-20-14-02-56.png)

# 2. 거대 언어 모델의 학습

### GPT-3: "거대 언어 모델의 시초"
- 가장 큰 버전의 GPT-3: 1750억 개의 매개변수 -> 이전 언어 모델 대비 최소 10배 이상 큰 모델
- 본격적으로 인-컨텍스트 학습(in-context learning) 능력이 나타나기 시작한 언어 모델

  ![1020_텍스트_파운데이션_모델_2025-10-20-14-04-07](images/1020_텍스트_파운데이션_모델_2025-10-20-14-04-07.png)

- 학습 방법: 다음 토큰 예측(Next token prediction)
- 학습 데이터: 3000억 토큰(4TB 텍스트 데이터 = 인터넷 + 양질의 텍스트북)
- 학습 비용: 150억원 정도로 추산

  ![1020_텍스트_파운데이션_모델_2025-10-20-14-05-04](images/1020_텍스트_파운데이션_모델_2025-10-20-14-05-04.png)

### 다음 토큰 예측 기반 거대 언어 모델의 한계
- 사람의 지시에 대해 올바르지 않은 응답을 생성하거나, 유해한 응답을 생성할 수도 있음

  ![1020_텍스트_파운데이션_모델_2025-10-20-14-05-46](images/1020_텍스트_파운데이션_모델_2025-10-20-14-05-46.png)

### 정렬(Alignment) 학습: 거대 언어 모델의 출력이 사용자의 의도와 가치를 반영하도록 하는 것
- (1) 지시 학습(instruction tuning): 주어진 지시에 대해 **어떤 응답이 생성**되어야 하는지
- (2) 선호 학습(preference learning): 상대적으로 어떤 **응답이 더 선호**되어야 하는지

  ![1020_텍스트_파운데이션_모델_2025-10-20-14-08-09](images/1020_텍스트_파운데이션_모델_2025-10-20-14-08-09.png)

## 1. 지시 학습(Instruction Tuning)

### 지시 학습(Instruction Tuning): 주어진 지시에 대해 어떤 응답이 생성되어야 하는지
- 학습 방법 자체는 기존 언어 모델(e.g. BERT)에서의 지도 추가 학습(supervised fine-tuning, SFT)과 동일
- Remark. 기존 언어 모델은 각 태스크(e.g. 감정 분석)마다 **별도의 추가 학습 및 모델을 저장**

  ![1020_텍스트_파운데이션_모델_2025-10-20-14-09-41](images/1020_텍스트_파운데이션_모델_2025-10-20-14-09-41.png)

- Idea: 모든 자연어 태스크는 텍스트 기반 지시(instruction)와 응답으로 표현할 수 있지 않을까?
  - 예시 #1. 감정 분석: "주어진 리뷰 속 유저의 감정이 긍정적이야, 부정적이야?"
  - 예시 #2. 번역: "주어진 문장을 영어로 번역해줘."

  ![1020_텍스트_파운데이션_모델_2025-10-20-14-10-48](images/1020_텍스트_파운데이션_모델_2025-10-20-14-10-48.png)

### 지시 학습: 거대 언어 모델을 다양한 지시 기반 입력과 이에 대한 응답으로 추가학습(FLAN)
- 학습 방법: 주어진 입력을 받아서 이에 대한 응답을 따라 하도록 지도 추가 학습(Supervised Fine-Tuning, SFT)

  ![1020_텍스트_파운데이션_모델_2025-10-20-14-11-48](images/1020_텍스트_파운데이션_모델_2025-10-20-14-11-48.png)

- 학습 데이터의 다양성 증대를 위해 각 태스크를 다양한 지시(템플릿)로 표현할 수 있음

  ![1020_텍스트_파운데이션_모델_2025-10-20-14-12-24](images/1020_텍스트_파운데이션_모델_2025-10-20-14-12-24.png)

- 기존 NLP 태스크 데이터를 지시 학습을 위한 데이터로 수정하여 학습 및 태스크에 활용
- 학습 시에 보지 못 한 지시에 대한 일반화 성능 평가를 위해 관련 없는 태스크들을 테스트에 별도로 활용
  - 예시: 요약(summarization)을 테스트 때의 태스크로 활용하기 위하여. 학습 시에는 제거

  ![1020_텍스트_파운데이션_모델_2025-10-20-14-13-43](images/1020_텍스트_파운데이션_모델_2025-10-20-14-13-43.png)

- 실험 결과: 예시 없이도(0-shot) 새로운 지시에 대해 올바른 응답을 내놓는 성능이 크게 증가
  - LaMDA-PT 137B 라는 구글의 당시 기준 SOTA 텍스트 파운데이션 모델을 추가 학습

  ![1020_텍스트_파운데이션_모델_2025-10-20-14-15-18](images/1020_텍스트_파운데이션_모델_2025-10-20-14-15-18.png)

### 지시 학습: 파운데이션 모델을 다양한 지시 기반 입력과 이에 대한 응답으로 추가학습(FLAN or T0)
- 실험 결과: 성능 향상을 위한 핵심 요소는 다음과 같음
  1. 학습 태스크의 개수: 다양한 종류의 지시를 학습할 수록 보지 못 한 지시에 대한 일반화 성능이 좋아짐
  2. 추가 학습하는 모델의 크기: 특정 규모 이하에서는 지시 학습의 효과성이 떨어짐 -> 지시를 이해하고 응답하는 것도 창발성의 하나
  3. 지시를 주는 방법: 자연어 지시로 사람에게 대화하듯 지시하는 것이 가장 효과적

  ![1020_텍스트_파운데이션_모델_2025-10-20-14-17-23](images/1020_텍스트_파운데이션_모델_2025-10-20-14-17-23.png)

## 2. 선호 학습(Preference Learning)

### 지시 학습의 한계: 주어진 입력에 대해 적절한 하나의 응답이 있다고 가정
- 이는 정답이 정해져 있는 객관적 태스크(e.g. 수학)에서는 자연스러움

  ![1020_텍스트_파운데이션_모델_2025-10-20-14-19-11](images/1020_텍스트_파운데이션_모델_2025-10-20-14-19-11.png)

- 그러나, 정답이 정해져 있지 않은 개방형(Open-ended) 태스크(e.g. 번역)에서는 한계가 있음
  - Goal: 단순히 복수 정답을 허락하는 대신 더 좋은(선호되는) 응답을 생성하도록 하고 싶음

  ![1020_텍스트_파운데이션_모델_2025-10-20-14-20-40](images/1020_텍스트_파운데이션_모델_2025-10-20-14-20-40.png)

### 선호 학습(Preference Learning): 다양한 응답 중 사람이 더 선호하는 응답을 생성하도록 추가학습
- 다양한 응답은 모델이 생성, 응답간의 선호도는 사람이 제공

  ![1020_텍스트_파운데이션_모델_2025-10-20-14-21-24](images/1020_텍스트_파운데이션_모델_2025-10-20-14-21-24.png)

- ChatGPT를 만들기 위한 핵심 알고리즘
  - 공식 문서는 공개되어 있지 않지만, 아래와 같은 힌트가 공식 블로그에 제공되어 있음

    ![1020_텍스트_파운데이션_모델_2025-10-20-14-22-11](images/1020_텍스트_파운데이션_모델_2025-10-20-14-22-11.png)

- InstructGPT의 핵심 아이디어
  - 사람의 피드백을 통한 강화학습(Reinforcement Learning from Human Feedback, RLHF)
    - 사람의 피드백 ≒ 응답에 대한 선호도

    ![1020_텍스트_파운데이션_모델_2025-10-20-14-24-20](images/1020_텍스트_파운데이션_모델_2025-10-20-14-24-20.png)

### InstructGPT의 학습 방법: Step 1. 지시 학습을 통한 텍스트 파운데이션 모델(e.g. GPT-3)의 추가 학습
- 실제 유저로부터 다양한 지시 입력을 수집하고, 해당 입력에 대해 훈련된 사람 주석자들이 정답 데이터를 생성

  ![1020_텍스트_파운데이션_모델_2025-10-20-14-27-04](images/1020_텍스트_파운데이션_모델_2025-10-20-14-27-04.png)

### InstructGPT의 학습 방법: Step 2. 사람의 선호 데이터를 수집하여, 보상 모델(Reward model, RM)을 학습
- 주어진 입력에 대한 선택지는 모델이 생성, 다양한 선택지에 대한 선호도는 사람이 생성
- 사람과 일치한 선호도를 출력할 수 있도록 보상 모델을 지도 학습
  - 사람이 선호하는 응답이 입력으로 주어짐 -> 높은 보상을 출력

  ![1020_텍스트_파운데이션_모델_2025-10-20-14-42-47](images/1020_텍스트_파운데이션_모델_2025-10-20-14-42-47.png)

  ![1020_텍스트_파운데이션_모델_2025-10-20-14-43-09](images/1020_텍스트_파운데이션_모델_2025-10-20-14-43-09.png)

### InstructGPT의 학습 방법: Step 3. 보상이 높은 응답을 생성하도록 강화 학습을 통해 추가 학습
- 핵심: Step 1&2에서 보지 못한 질문에 대해 사람의 추가적인 개입 없이 학습된 모델들을 통해 추가 학습이 진행
- 지시 학습된 모델을 보상 모델 기반 강화 학습을 통해 한 번 더 추가 학습

  ![1020_텍스트_파운데이션_모델_2025-10-20-14-44-51](images/1020_텍스트_파운데이션_모델_2025-10-20-14-44-51.png)

### InstructGPT의 결과: 유저의 지시를 얼마나 잘 수행하는 지를 사람이 직접 평가
- 단순 프롬프팅이나 지시 학습에 비해 발전된 지시 수행능력을 보여줌

  ![1020_텍스트_파운데이션_모델_2025-10-20-14-45-31](images/1020_텍스트_파운데이션_모델_2025-10-20-14-45-31.png)

### InstructGPT의 결과: 얼마나 안전한 응답을 생성하는지 평가
- 기존 대비, InstructGPT는 해로운 응답(RalToxicity)과 거짓말(TruthfulQA, Hallucinations)을 덜 생성

  ![1020_텍스트_파운데이션_모델_2025-10-20-14-46-38](images/1020_텍스트_파운데이션_모델_2025-10-20-14-46-38.png)

### LLaMA2
- InstructGPT와 비슷하게 RLHF와 대화 데이터를 활용한 LLaMA2 Chat 모델을 공개
  - LLaMA1 때에는 사전 학습된 모델만 공개

  ![1020_텍스트_파운데이션_모델_2025-10-20-14-47-28](images/1020_텍스트_파운데이션_모델_2025-10-20-14-47-28.png)

- 당시 대화형 타입 개방형 거대 언어 모델 중 가장 우수한 성능을 보여줌

  ![1020_텍스트_파운데이션_모델_2025-10-20-14-47-53](images/1020_텍스트_파운데이션_모델_2025-10-20-14-47-53.png)

# 3. 거대 언어 모델의 추론

## 1. 디코딩(Decoding) 알고리즘

### 거대 언어 모델의 자동회귀 생성(Auto-regressive Generation)
- 학습이 완료된 거대 언어 모델은 어떻게 응답을 생성할까? -> 순차적 추론을 통한 "**토큰별 생성**"

  ![1020_텍스트_파운데이션_모델_2025-10-20-14-50-03](images/1020_텍스트_파운데이션_모델_2025-10-20-14-50-03.png).
  
- Q. 언제 추론 및 토큰 생성을 멈추고 응답을 제공?
  - A. EOS 토큰 생성 시 종료 or 사전에 정의된 토큰 수 도달 시 종료
    - E.g. [SEP]: BERT에서 사용된 EOS(End of Sentence) 토큰

    ![1020_텍스트_파운데이션_모델_2025-10-20-14-51-41](images/1020_텍스트_파운데이션_모델_2025-10-20-14-51-41.png)

- Goal: 주어진 입력 x = [x_1, ..., x_L] 에 대해 다음 토큰 x_L+1 을 생성
  - 거대 언어 모델: 입력 x에 대해 다음 토큰에 대한 확률 분포 p-hat(x)를 제공

- 디코딩(Decoding) 알고리즘: p-hat(x)로부터 x_L+1을 생성하는 알고리즘(다음 단어를 선택하는 방법)

  ![1020_텍스트_파운데이션_모델_2025-10-20-14-57-21](images/1020_텍스트_파운데이션_모델_2025-10-20-14-57-21.png)

- Pytorch 실제 예시

  ![1020_텍스트_파운데이션_모델_2025-10-20-14-57-39](images/1020_텍스트_파운데이션_모델_2025-10-20-14-57-39.png)

  ![1020_텍스트_파운데이션_모델_2025-10-20-14-57-56](images/1020_텍스트_파운데이션_모델_2025-10-20-14-57-56.png)

### 디코딩(Decoding) 알고리즘: 1. Greedy Decoding
- 핵심 아이디어: 가장 확률이 높은 다음 토큰을 선택
  - 장점: 사용하기 쉽다.
  - 단점: 직후만 고려하기 때문에 생성 응답이 최종적으로 최선이 아닐 수 있다.

    ![1020_텍스트_파운데이션_모델_2025-10-20-14-58-50](images/1020_텍스트_파운데이션_모델_2025-10-20-14-58-50.png)
  
### 디코딩(Decoding) 알고리즘: 2. Beam Search
- 핵심 아이디어: 확률이 높은 k개(beam size)의 후보를 동시에 고려
  - 고르는 기준: 누적 생성 확률(지금까지 생성한 문장 전체가 나올 확률의 곱)
  - 앞선 Greedy Decoding은 매 시점마다 가장 높은 확률의 선택지 1개 만을 선택했지만, Beam Search는 전체 문장 후보들의 누적 확률을 기준으로 상위 k개를 남기는 것

    ![1020_텍스트_파운데이션_모델_2025-10-20-15-00-28](images/1020_텍스트_파운데이션_모델_2025-10-20-15-00-28.png)

  - 장점: 최종적으로 좋은 응답 생성 확률이 높다.
  - 단점: 계산 비용이 많이 늘어난다(각 후보마다 LLM 추론을 수행)

    ![1020_텍스트_파운데이션_모델_2025-10-20-15-01-33](images/1020_텍스트_파운데이션_모델_2025-10-20-15-01-33.png)

### 디코딩(Decoding) 알고리즘: 3. Sampling
- 핵심 아이디어: 거대 언어 모델이 제공한 확률을 기준으로 랜덤하게 생성
  - 장점: 다양한 응답을 생성할 수 있음
  - 단점: 생성된 응답의 품질이 감소할 수 있음

    ![1020_텍스트_파운데이션_모델_2025-10-20-15-02-44](images/1020_텍스트_파운데이션_모델_2025-10-20-15-02-44.png)

### 디코딩(Decoding) 알고리즘: 4. Sampling "with Temperature"
- 핵심 아이디어: 하이퍼 파라미터 T를 통해 거대 언어 모델이 생성한 확률 분포를 임의로 조작
  - T > 1: 확률 분포를 Smooth하게 만듦(더 다양한 응답 생성), T < 1: 확률 분포를 Sharp하게 만듦(기존에 확률이 높은 응답에 집중)

    ![1020_텍스트_파운데이션_모델_2025-10-20-15-04-13](images/1020_텍스트_파운데이션_모델_2025-10-20-15-04-13.png)

    ![1020_텍스트_파운데이션_모델_2025-10-20-15-04-32](images/1020_텍스트_파운데이션_모델_2025-10-20-15-04-32.png)

    ![1020_텍스트_파운데이션_모델_2025-10-20-15-04-47](images/1020_텍스트_파운데이션_모델_2025-10-20-15-04-47.png)

    ![1020_텍스트_파운데이션_모델_2025-10-20-15-06-04](images/1020_텍스트_파운데이션_모델_2025-10-20-15-06-04.png)

### 디코딩(Decoding) 알고리즘: 5. Top-K Sampling
- 핵심 아이디어: 확률이 높은 K개의 토큰들 중에서만 랜덤하게 확률에 따라 샘플링
  - 장점: 품질이 낮은 응답을 생성할 가능성을 줄일 수 있음

    ![1020_텍스트_파운데이션_모델_2025-10-20-15-06-52](images/1020_텍스트_파운데이션_모델_2025-10-20-15-06-52.png)

  - 단점: 확률 분포의 모양에 상관 없이 고정된 K개의 후보군을 고려

    ![1020_텍스트_파운데이션_모델_2025-10-20-15-07-22](images/1020_텍스트_파운데이션_모델_2025-10-20-15-07-22.png)

    ![1020_텍스트_파운데이션_모델_2025-10-20-15-07-39](images/1020_텍스트_파운데이션_모델_2025-10-20-15-07-39.png)

### 디코딩(Decoding) 알고리즘: 6. Top-P Sampling (or Nucleus Sampling)
- 핵심 아이디어: K를 고정하는 대신, 누적 확률(P)에 집중하여 K를 자동으로 조절
  - 예시: P=0.9 -> 확률이 높은 K개의 응답 후보의 확률을 더했을 때 0.9를 처음으로 초과하는 K를 사용

    ![1020_텍스트_파운데이션_모델_2025-10-20-15-09-11](images/1020_텍스트_파운데이션_모델_2025-10-20-15-09-11.png)

  - 다양한 평가 지표에서 기존 디코딩 알고리즘들 대비 좋은 성능을 달성

    ![1020_텍스트_파운데이션_모델_2025-10-20-15-09-37](images/1020_텍스트_파운데이션_모델_2025-10-20-15-09-37.png)

### 디코딩(Decoding) 알고리즘 별 장단점 요약

![1020_텍스트_파운데이션_모델_2025-10-20-15-10-01](images/1020_텍스트_파운데이션_모델_2025-10-20-15-10-01.png)

### 디코딩(Decoding) 알고리즘: 실제 예시 with ChatGPT

![1020_텍스트_파운데이션_모델_2025-10-20-15-10-27](images/1020_텍스트_파운데이션_모델_2025-10-20-15-10-27.png)

![1020_텍스트_파운데이션_모델_2025-10-20-15-10-45](images/1020_텍스트_파운데이션_모델_2025-10-20-15-10-45.png)

## 2. 프롬프트 엔지니어링

### 입력 프롬프트 = (1) 지시(instruction) + (2) 예시(few-shot examples)

![1020_텍스트_파운데이션_모델_2025-10-20-15-11-29](images/1020_텍스트_파운데이션_모델_2025-10-20-15-11-29.png)

- 어떻게 지시를 주는지, 어떤 예시를 보여주는 지가 거대 언어 모델의 성능에 크게 영향을 미침

  ![1020_텍스트_파운데이션_모델_2025-10-20-15-12-05](images/1020_텍스트_파운데이션_모델_2025-10-20-15-12-05.png)

  - 프롬프트 엔지니어링: 원하는 답을 얻기 위해 모델에 주어지는 입력(프롬프트)을 설계, 조정하는 기법

### 프롬프트 엔지니어링: 지시(Instruction)
- 감정 분류와 같은 쉬운 문제 뿐만 아니라 수학, 코딩과 같은 어려운 문제르 거대 언어 모델로 푸는 것에 많은 관심 집중
  - 예시: 수학 질의 응답(GSM8K -> 미국 초등학교 고학년 수준 수학 문제)

    ![1020_텍스트_파운데이션_모델_2025-10-20-15-13-59](images/1020_텍스트_파운데이션_모델_2025-10-20-15-13-59.png)

    ![1020_텍스트_파운데이션_모델_2025-10-20-15-14-12](images/1020_텍스트_파운데이션_모델_2025-10-20-15-14-12.png)

### Chain-of-Thought (CoT) 프롬프팅
- 아이디어: 단순히 질문과 응답만을 예시로 활용하는 것이 아니라, 추론(Reasoning) 과정도 예시에 포함
  - 이를 통해 테스트 질문에 대해 추론을 생성하고 응답하도록 유도함으로써, 더 정확한 정답 생성을 기대할 수 있음

    ![1020_텍스트_파운데이션_모델_2025-10-20-15-15-12](images/1020_텍스트_파운데이션_모델_2025-10-20-15-15-12.png)

- 결과: CoT는 거대 언어 모델(PaLM)의 추론 성능을 크게 증가시킴
  - PaLM: 당시 구글에서 사용했던 가장 큰 거대 언어 모델 (PaLM (540B) vs GPT-3 (175B))
- CoT로 인한 성능 향상은 모델 크기가 커질 수록 더 확대됨 (추론 ~= 창발성?)
  - 창발성: 모델 크기가 커지면 갑자기 새로운 능력이 나타나는 현상을 의미 (PaLM의 창발적 능력이 발현되었을 수 있음)

  ![1020_텍스트_파운데이션_모델_2025-10-20-15-16-47](images/1020_텍스트_파운데이션_모델_2025-10-20-15-16-47.png)

- 다른 추론 태스트: 마지막 단어 연결
  - In-domain: 예시도 2 단어, 테스트도 2 단어
  - Out-of-domain: 예시는 2 단어, 테스트는 4 단어

  ![1020_텍스트_파운데이션_모델_2025-10-20-15-17-39](images/1020_텍스트_파운데이션_모델_2025-10-20-15-17-39.png)

  ![1020_텍스트_파운데이션_모델_2025-10-20-15-17-55](images/1020_텍스트_파운데이션_모델_2025-10-20-15-17-55.png)

- 예시 기반 CoT는 강력하지만, 예시를 위한 추론 과정을 수집해야 되는 문제가 있음
- Q. 예시 없이도(0-shot) 거대 언어 모델의 추론 성능을 강화할 수 있을까(i.e., 0-shot CoT)?

  ![1020_텍스트_파운데이션_모델_2025-10-20-15-19-04](images/1020_텍스트_파운데이션_모델_2025-10-20-15-19-04.png)

  ![1020_텍스트_파운데이션_모델_2025-10-20-15-19-18](images/1020_텍스트_파운데이션_모델_2025-10-20-15-19-18.png)

### 0-shot CoT 프롬프팅
- 1. 유인 문장을 통한 추론 생성(e.g. Let's think step by step)

  ![1020_텍스트_파운데이션_모델_2025-10-20-15-20-00](images/1020_텍스트_파운데이션_모델_2025-10-20-15-20-00.png)

- 2. 주어진 질문과 생성된 추론을 통한 정답 생성(e.g. Therefore, the answer is)

  ![1020_텍스트_파운데이션_모델_2025-10-20-15-20-28](images/1020_텍스트_파운데이션_모델_2025-10-20-15-20-28.png)

- 결과: 0-shot CoT는 기존 0-shot 프롬프팅보다 훨씬 높은 추론 성능을 달성
- 또한, 0-shot CoT는 모델 크기가 임계점을 넘어서야 효과성이 발휘
  - 따라서, 추론 능력은 거대 언어 모델의 창발성 결과라고 볼 수 있음

  ![1020_텍스트_파운데이션_모델_2025-10-20-15-21-23](images/1020_텍스트_파운데이션_모델_2025-10-20-15-21-23.png)

- Q. 추출 문장의 중요성?

  ![1020_텍스트_파운데이션_모델_2025-10-20-15-21-42](images/1020_텍스트_파운데이션_모델_2025-10-20-15-21-42.png)

# 4. 거대 언어 모델의 평가와 응용

## 1. 거대 언어 모델의 평가

### 평가(Evaluation): 구축한 시스템(e.g. 코드 or 앱)이 실제로 잘 동작하는 지를 확인하는 단계
- 평가의 3가지 요소
  1) 목표: 시스템으로 무엇을 달성하고자 하는지
  2) 평가 방법: 어떤 방법으로 평가할 것인지
  3) 평가 지표: 어떻게 성공 여부를 판단할 것인지

- 예시: 배달 어플
  1) 목표: 음식을 음식점으로부터 유저에게까지 배달하는 것
  2) 평가 방법: 배달 시간을 측정
  3) 평가 지표: 전체 유저 배달 건수에 대한 평균 배달 시간

### AI 모델의 평가: "테스트 데이터"
- 핵심 가정: 학습 단계에서 본 적이 없고, 질문과 정답을 알고 있음
- 예시: 감정 분류
  1) 목표: 주어진 입력 텍스트의 감정을 올바르게 예측하는 것
  2) 평가 방법: AI 모델의 예측 감정과 사람이 작성한 정답을 비교하는 것
  3) 평가 지표: 테스트 데이터 셋에서의 평균 정확도

  ![1020_텍스트_파운데이션_모델_2025-10-20-15-25-11](images/1020_텍스트_파운데이션_모델_2025-10-20-15-25-11.png)

### 거대 언어 모델 평가의 특징
- 특정 태스크에서 학습된 기존 AI 모델들과 달리, 거대 언어 모델은 다양한 태스크에 대해 동시에 학습됨
  - 따라서 거대 언어 모델의 성능을 올바르게 평가하기 위해서는 많은 태스크에서의 성능을 종합적으로 판단해야 함
  - 또한 디코딩 알고리즘, 입력 프롬프트에 따라 같은 질문에 대해서도 예측이 바뀌므로 공평한 비교를 위해서는 해당 부분도 고려해야함

    ![1020_텍스트_파운데이션_모델_2025-10-20-15-26-22](images/1020_텍스트_파운데이션_모델_2025-10-20-15-26-22.png)

    ![1020_텍스트_파운데이션_모델_2025-10-20-15-26-40](images/1020_텍스트_파운데이션_모델_2025-10-20-15-26-40.png)

    ![1020_텍스트_파운데이션_모델_2025-10-20-15-26-59](images/1020_텍스트_파운데이션_모델_2025-10-20-15-26-59.png)  

    ![1020_텍스트_파운데이션_모델_2025-10-20-15-27-15](images/1020_텍스트_파운데이션_모델_2025-10-20-15-27-15.png)

### 거대 언어 모델 평가 방법의 종류
- 정답이 정해진 경우
  - 평가 방법: 예측과 정답을 비교하여 일치도를 측정(정확도 (Accuracy))

- 정답이 정해져 있지 않은 경우  
  - 평가 방법 #1: 사람이 임의의 정답을 작성 및 이와 예측을 비교
  - 평가 방법 #2: 정답과 무관하게 생성 텍스트 자체의 품질만을 측정
  - 평가 방법 #3: 생성된 텍스트의 "상대적 선호"를 평가

### 정답이 정해져 있는 경우
- 예측과 정답을 비교하여 일치도를 측정 -> "정확도(Accuracy)"
  - 예시: MMLU(Massive Mulittask Language Understanding) 벤치마크 -> 57개의 다양한 전문 분야에 대한 객관식 문제들로 구성

    ![1020_텍스트_파운데이션_모델_2025-10-20-15-34-09](images/1020_텍스트_파운데이션_모델_2025-10-20-15-34-09.png)

### 정답이 정해져 있지 않은 경우
- 예시 1: 문서 요약

  ![1020_텍스트_파운데이션_모델_2025-10-20-15-34-43](images/1020_텍스트_파운데이션_모델_2025-10-20-15-34-43.png)

- 예시 2: 스토리 생성

  ![1020_텍스트_파운데이션_모델_2025-10-20-15-35-00](images/1020_텍스트_파운데이션_모델_2025-10-20-15-35-00.png)

- 평가 방법 #1: 사람이 임의의 정답을 작성 및 이와 예측을 비교
  - Q. 어떻게 두 텍스트 간의 유사도를 측정할 수 있을까?

    ![1020_텍스트_파운데이션_모델_2025-10-20-15-35-39](images/1020_텍스트_파운데이션_모델_2025-10-20-15-35-39.png)
  
  - A. 예시 1) 단어 수준에서의 유사도 측정(e.g. ROUGE)

    ![1020_텍스트_파운데이션_모델_2025-10-20-15-36-11](images/1020_텍스트_파운데이션_모델_2025-10-20-15-36-11.png)

  - A. 예시 2) 벡터 공간에서의 유사도 측정(e.g. cosine similarity on embedding space)

    ![1020_텍스트_파운데이션_모델_2025-10-20-15-37-07](images/1020_텍스트_파운데이션_모델_2025-10-20-15-37-07.png)

- 평가 방법 #2: 정답과 무관하게 생성 텍스트 자체의 품질만을 측정
- 예시: Perplexity(PPL) -> 얼마나 문장이 확률적으로 자연스러운지 측정
  - 다음 단어가 올 확률을 계산하기 위해 언어모델을 주로 활용(예: GPT-2)

    ![1020_텍스트_파운데이션_모델_2025-10-20-15-38-04](images/1020_텍스트_파운데이션_모델_2025-10-20-15-38-04.png)

- 단순한 등장 확률이 아니라, 여러 가지 측면에서 평가를 하고 싶다면?
  - 예: 창의성, 유창성, 가독성 등.. -> 각각에 부합하는 평가지표를 따로 설계하는 것은 굉장히 어려움
  - 기존 해결 방법: 전문가를 고용하여 평가를 맡김<br>
  -> 거대 언어 모델로 해당 역할을 대신 수행하게 하면 안 될까?

- 평가 방법 #3: 생성 텍스트의 "상대적 선호"를 평가할 수도 있음

  ![1020_텍스트_파운데이션_모델_2025-10-20-15-43-01](images/1020_텍스트_파운데이션_모델_2025-10-20-15-43-01.png)

- 대표 예시: LMArena
  - 실제 유저 피드백을 활용하였으며, 거대 언어 모델 성능 측정 방법 중 가장 신뢰성 있는 방법 중 하나로 여겨짐

    ![1020_텍스트_파운데이션_모델_2025-10-20-15-43-48](images/1020_텍스트_파운데이션_모델_2025-10-20-15-43-48.png)

    ![1020_텍스트_파운데이션_모델_2025-10-20-15-44-03](images/1020_텍스트_파운데이션_모델_2025-10-20-15-44-03.png)

    ![1020_텍스트_파운데이션_모델_2025-10-20-15-44-20](images/1020_텍스트_파운데이션_모델_2025-10-20-15-44-20.png)

    ![1020_텍스트_파운데이션_모델_2025-10-20-15-44-38](images/1020_텍스트_파운데이션_모델_2025-10-20-15-44-38.png)

    ![1020_텍스트_파운데이션_모델_2025-10-20-15-44-54](images/1020_텍스트_파운데이션_모델_2025-10-20-15-44-54.png)

  - 그러나 **높은 평가 비용 및 시간을 필요로 함**
    - **거대 언어 모델로 대체한다면?**

      ![1020_텍스트_파운데이션_모델_2025-10-20-15-45-52](images/1020_텍스트_파운데이션_모델_2025-10-20-15-45-52.png)


### 거대 언어 모델을 활용한 평가
- LLM-as-judge (or G-Eval): 거대 언어 모델을 통해 생성 텍스트를 평가
  1. 유저는 다음과 같은 정보를 제공
    1) 풀고자 하는 태스크(e.g. 질문)
    2) 평가하고자 하는 텍스트
    3) 평가 기준을 제공

    ![1020_텍스트_파운데이션_모델_2025-10-20-15-40-09](images/1020_텍스트_파운데이션_모델_2025-10-20-15-40-09.png)

  2. 거대 언어 모델은 평가 결과(점수, 이유)를 제공

    ![1020_텍스트_파운데이션_모델_2025-10-20-15-40-40](images/1020_텍스트_파운데이션_모델_2025-10-20-15-40-40.png)

- GPT-4를 활용한 평가는 기존 평가 지표(e.g. ROUGE)보다 더 사람과 유사한 결과를 보임

  ![1020_텍스트_파운데이션_모델_2025-10-20-15-41-17](images/1020_텍스트_파운데이션_모델_2025-10-20-15-41-17.png)

- LLaMA3 학습을 위한 데이터 필터링에도 사용되는 등 다양한 어플리케이션에서 이미 활용되고 있음

  ![1020_텍스트_파운데이션_모델_2025-10-20-15-41-54](images/1020_텍스트_파운데이션_모델_2025-10-20-15-41-54.png)

  ![1020_텍스트_파운데이션_모델_2025-10-20-15-42-08](images/1020_텍스트_파운데이션_모델_2025-10-20-15-42-08.png)

- LLM-as-judge (or G-Eval): 거대 언어 모델을 통해 생성 텍스트의 "상대적 선호"를 평가할 수도 있음

  ![1020_텍스트_파운데이션_모델_2025-10-20-15-46-41](images/1020_텍스트_파운데이션_모델_2025-10-20-15-46-41.png)

- 그러나, 해당 평가 방식은 몇 가지 한계점을 가지고 잇음
  1. **위치 편향**: 특정 위치의 응답을 상대적으로 선호(e.g. 첫 번째 응답 > 두 번째 응답)

    ![1020_텍스트_파운데이션_모델_2025-10-20-15-47-28](images/1020_텍스트_파운데이션_모델_2025-10-20-15-47-28.png)

  2. **길이 편향**: 품질과 무관하게 길이가 긴 응답을 상대적으로 선호

    ![1020_텍스트_파운데이션_모델_2025-10-20-15-47-56](images/1020_텍스트_파운데이션_모델_2025-10-20-15-47-56.png)

    ![1020_텍스트_파운데이션_모델_2025-10-20-15-48-14](images/1020_텍스트_파운데이션_모델_2025-10-20-15-48-14.png)

  3. **자기 선호 편향**: 생성 모델이 평가 모델과 같은 경우, 이를 선호

    ![1020_텍스트_파운데이션_모델_2025-10-20-15-48-41](images/1020_텍스트_파운데이션_모델_2025-10-20-15-48-41.png)

  - 한계점을 인지하고 보완하여 활용하는 것이 필요
    - 위치 편향은 순서를 바꿔서 두 번 평가하고 평균을 취하는 것으로 해결할 수 있음
    - 길이 편향은 길이가 미치는 영향을 통계적으로 제거해서 어느 정도 해결할 수 있음(e.g. AlpacaEval2: GPT-4를 평가자로 활용 및 길이 영향 제거)

      ![1020_텍스트_파운데이션_모델_2025-10-20-15-50-11](images/1020_텍스트_파운데이션_모델_2025-10-20-15-50-11.png)

## 2. 거대 언어 모델의 응용 및 한계

### 거대 언어 모델의 응용: 멀티모달 파운데이션 모델
- 예시: GPT-4o -> 멀티모달 입력(이미지, 비디오, 오디오), 멀티모달(오디오, 텍스트) 출력 생성

  ![1020_텍스트_파운데이션_모델_2025-10-20-15-51-24](images/1020_텍스트_파운데이션_모델_2025-10-20-15-51-24.png)

  - X_v(이미지 입력) -> Vision Encoder -> Z_v(이미지 특징 벡터) -> Projection W(이미지 투영 행렬) -> H_v(이미지 토큰 표현)
  - X_q(텍스트 질문) -> Tokenization -> H_q(텍스트 토큰 표현)
  - (H_v, H_q) -> fΦ(Language Model) -> X_a(최종 응답 텍스트)
- 핵심 아이디어: 다른 모달리티 데이터를 거대 언어 모델이 이해할 수 있도록 토큰화 및 추가 학습

  ![1020_텍스트_파운데이션_모델_2025-10-20-15-53-27](images/1020_텍스트_파운데이션_모델_2025-10-20-15-53-27.png)

  ![1020_텍스트_파운데이션_모델_2025-10-20-15-53-44](images/1020_텍스트_파운데이션_모델_2025-10-20-15-53-44.png)

  - X_v(사람의 물체 옮기는 행동 비디오) -> Vision-Language Model(행동 의미 이해 + 제어 신호 변환) -> X_a(로봇의 물체 전달 행동)

    ![1020_텍스트_파운데이션_모델_2025-10-20-15-54-57](images/1020_텍스트_파운데이션_모델_2025-10-20-15-54-57.png)

### 거대 언어 모델의 응용: 합성 데이터 생성
- Self-instruct: 175개의 데이터를 사람이 작성한 뒤, GPT-3를 통해 52000개의 합성 데이터 생성
- Self-instruct 의의
  - 사람이 만든 소량의 데이터를 기반으로 대규모 합성 데이터셋을 확장
  - 합성 데이터로 학습한 모델이 사람 데이터 기반 성능과 유사한 결과 달성

    ![1020_텍스트_파운데이션_모델_2025-10-20-15-57-08](images/1020_텍스트_파운데이션_모델_2025-10-20-15-57-08.png)

  ![1020_텍스트_파운데이션_모델_2025-10-20-15-57-29](images/1020_텍스트_파운데이션_모델_2025-10-20-15-57-29.png)

  ![1020_텍스트_파운데이션_모델_2025-10-20-15-57-46](images/1020_텍스트_파운데이션_모델_2025-10-20-15-57-46.png)

- 필터링 의의
  - 합성 데이터에서 중복, 유사(유사도 높은) 데이터 제거
  - 모델이 더 폭 넓은 지시문 학습이 가능하도록 다양한 Instruction 확보

    ![1020_텍스트_파운데이션_모델_2025-10-20-15-58-37](images/1020_텍스트_파운데이션_모델_2025-10-20-15-58-37.png)

- 필터링 결과: 중복이거나 무관한 합성 데이터는 제거, 새로운 지시문만 남겨 다양성 확보

  ![1020_텍스트_파운데이션_모델_2025-10-20-15-59-18](images/1020_텍스트_파운데이션_모델_2025-10-20-15-59-18.png)

- 결과: 기존 InstructGPT에서 활용된 사람이 만든 데이터와 비슷한 성능 달성
  - GPT-3 + 합성 데이터 39.9 vs 사람 데이터 기반 40.8
  - SuperNI 함께 학습 시: 합성 데이터 51.6 > 사람 데이터 기반 49.5

    ![1020_텍스트_파운데이션_모델_2025-10-20-16-00-18](images/1020_텍스트_파운데이션_모델_2025-10-20-16-00-18.png)

- 해당 방식은 LLaMA-1 기반 최초 개방형 instruction following 모델이 Alpaca 학습에 활용됨

  ![1020_텍스트_파운데이션_모델_2025-10-20-16-01-15](images/1020_텍스트_파운데이션_모델_2025-10-20-16-01-15.png)

- Alpagasus: 프롬프팅을 통한 합성 데이터의 품질 평가 및 필터링 제안
- Alpagasus 의의: 저품질 합성 데이터를 걸러내고 고품질만 학습해, 빠르고 강력한 성능 달성 목표

  ![1020_텍스트_파운데이션_모델_2025-10-20-16-02-07](images/1020_텍스트_파운데이션_모델_2025-10-20-16-02-07.png)

- 52000개의 Alpaca 합성 데이터 중 9000개 정도만 4.5 이상의 점수를 부여받음 -> 해당 데이터만 학습에 사용

  ![1020_텍스트_파운데이션_모델_2025-10-20-16-02-43](images/1020_텍스트_파운데이션_모델_2025-10-20-16-02-43.png)

- 결과: 전체 합성 데이터를 사용한 경우보다 높은 성능을 달성

  ![1020_텍스트_파운데이션_모델_2025-10-20-16-03-04](images/1020_텍스트_파운데이션_모델_2025-10-20-16-03-04.png)

### 거대 언어 모델의 한계: 환각(Hallucination)
- 사실과 다르거나 전적으로 지어낸 내용임에도 불구하고 정확한 정보와 <u>동일한 자신감과 유창함</u>으로 응답을 생성
  - 이는 거대 언어 모델이 확률적으로 다음 토큰 예측을 통해 응답을 생성하기 때문에 발생
  - 따라서, 사용자 입장에서 거대 언어 모델의 응답에 대한 진위성을 구별하기 어렵게 만듦

    ![1020_텍스트_파운데이션_모델_2025-10-20-16-04-20](images/1020_텍스트_파운데이션_모델_2025-10-20-16-04-20.png)

- 사전 학습 데이터의 제한적인 범위가 환각 현상의 원인이 되기도 함

  ![1020_텍스트_파운데이션_모델_2025-10-20-16-04-44](images/1020_텍스트_파운데이션_모델_2025-10-20-16-04-44.png)

  - 검색 증강 생성(Retrieval-augmented Generation, RAG)을 통해 해결 가능 -> 기본 기능으로 대부분의 거대 언어 모델 서비스에 탑재

    ![1020_텍스트_파운데이션_모델_2025-10-20-16-05-34](images/1020_텍스트_파운데이션_모델_2025-10-20-16-05-34.png)

### 거대 언어 모델의 한계: 탈옥(Jailbreaking)
- 프롬프팅 엔지니어링을 통해 거대 언어 모델의 정렬을 우회할 수 있다는 것이 확인됨
- 예시: Do Anything Now (DAN) 프롬프팅

  ![1020_텍스트_파운데이션_모델_2025-10-20-16-06-33](images/1020_텍스트_파운데이션_모델_2025-10-20-16-06-33.png)

  - GPT 답변: 특정 인물이나 사물에 대한 감정, 의견을 가질 수 없음
  - DAN 답변: 히틀러에 대해 감정을 담아 주관적인 의견을 표현함

    ![1020_텍스트_파운데이션_모델_2025-10-20-16-07-13](images/1020_텍스트_파운데이션_모델_2025-10-20-16-07-13.png)

- 여러 단계의 학습 과정에서 기인한 근본적인 한계 때문에 발생했으며, 다양한 탈옥/방어 방법이 활발히 탐구 중
  - 예시 (a): 모델의 안전 규칙과 프롬프트 지시의 충돌을 유도하여 규칙을 우회하는 사례
  - 예시 (b): 의미 없는 랜덤 토큰을 지시 패턴으로 잘못 일반화하도록 유도하여 규칙을 회피하는 사례

    ![1020_텍스트_파운데이션_모델_2025-10-20-16-09-10](images/1020_텍스트_파운데이션_모델_2025-10-20-16-09-10.png)

### 거대 언어 모델의 한게: AI 텍스트 검출
- 거대 언어 모델의 무분별한 사용이 학교 및 회사에서 여러 가지 새로운 문제를 만들고 있음

  ![1020_텍스트_파운데이션_모델_2025-10-20-16-09-53](images/1020_텍스트_파운데이션_모델_2025-10-20-16-09-53.png)

  - Q. 거대 언어 모델이 만든 텍스트를 구분 또는 탐지할 수 있을까?
  - A. 어느 정도 가능하다.

    ![1020_텍스트_파운데이션_모델_2025-10-20-16-11-03](images/1020_텍스트_파운데이션_모델_2025-10-20-16-11-03.png)